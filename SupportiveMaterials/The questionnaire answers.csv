Place I found the resource,"@inproceedings{pap1,
  title={Sa-net: Shuffle attention for deep convolutional neural networks},
  author={Zhang, Qing-Long and Yang, Yu-Bin},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2235--2239},
  year={2021},
  organization={IEEE}
}","@misc{resnet,
        author = {},
        title = {{R}es{N}et-50 convolutional neural network - {M}{A}{T}{L}{A}{B} resnet50 - {M}ath{W}orks {B}enelux --- nl.mathworks.com},
        howpublished = {\url{https://nl.mathworks.com/help/deeplearning/ref/resnet50.html}},
        year = {},
        note = {[Accessed 25-Jun-2023]},
}","@misc{imagenet1,
        author = {},
        title = {imagenet-1k · {D}atasets at {H}ugging {F}ace --- huggingface.co},
        howpublished = {\url{https://huggingface.co/datasets/imagenet-1k }},
        year = {},
        note = {[Accessed 25-Jun-2023]},
}","@inproceedings{coco,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}","@misc{amt,
	author = {},
	title = {{A}mazon {M}echanical {T}urk --- mturk.com},
	howpublished = {\url{https://www.mturk.com/}},
	year = {},
	note = {[Accessed 25-Jun-2023]},
}","@article{wordnet,
  title={Introduction to WordNet: An on-line lexical database},
  author={Miller, George A and Beckwith, Richard and Fellbaum, Christiane and Gross, Derek and Miller, Katherine J},
  journal={International journal of lexicography},
  volume={3},
  number={4},
  pages={235--244},
  year={1990},
  publisher={Oxford University Press}
}",pap2,"@inproceedings{wsj023,
title={Deep clustering: Discriminative embeddings for segmentation and separation},
author={Hershey, John R and Chen, Zhuo and Le Roux, Jonathan and Watanabe, Shinji},
booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages={31--35},
year={2016},
organization={IEEE}
}","@inproceedings{wsj0,
  title={Collection and Analyses of WSJ-CSR Data at MIT},
  author={Phillips, Michael and Glass, James and Polifroni, Joseph and Zue, Victor},
  booktitle={Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992},
  year={1992}
}",pap3,"@misc{datatang1,
        author = {Beijing DataTang Technology Co., Ltd},
        title = {{A}{I} data annotation data customization {D}atatang --- datatang.ai},
        howpublished = {\url{https://www.datatang.ai/annotation}},
        year = {},
        note = {[Accessed 25-Jun-2023]},
}","@inproceedings{aishell11,
  title={Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline},
  author={Bu, Hui and Du, Jiayu and Na, Xingyu and Wu, Bengu and Zheng, Hao},
  booktitle={2017 20th conference of the oriental chapter of the international coordinating committee on speech databases and speech I/O systems and assessment (O-COCOSDA)},
  pages={1--5},
  year={2017},
  organization={IEEE}
}","@article{aishell2,
  title={Aishell-2: Transforming mandarin asr research into industrial scale},
  author={Du, Jiayu and Na, Xingyu and Liu, Xuechen and Bu, Hui},
  journal={arXiv preprint arXiv:1808.10583},
  year={2018}
}","@article{aurora,
  title={Aurora working group: DSR front end LVCSR evaluation AU/384/02},
  author={Pearce, David and Picone, J},
  journal={Inst. for Signal \& Inform. Process., Mississippi State Univ., Tech. Rep},
  year={2002}
}","@misc{csj1,
        author = {Seiju Sugito, Kikuo Maekawa, Hanae Koiso, Kenya Nishikawa, Yoko Mabuchi        },
        title = {{D}ocuments - {C}orpus of {S}pontaneous {J}apanese --- clrd.ninjal.ac.jp},
        howpublished = {\url{https://clrd.ninjal.ac.jp/csj/en/document.html}},
        year = {2006},
        note = {[Accessed 25-Jun-2023]},
}","@misc{chime,
        author = {Kean Chin},
        title = {{T}he 4th {C}{H}i{M}{E} {S}peech {S}eparation and {R}ecognition {C}hallenge --- spandh.dcs.shef.ac.uk},
        howpublished = {\url{https://spandh.dcs.shef.ac.uk/chime_challenge/CHiME4/}},
        year = {2016},
        note = {[Accessed 25-Jun-2023]},
}","@article{fishhome1,
  title={Fisher and CALLHOME Spanish--English speech translation},
  author={Post, Matt and Kumar, Gaurav and Lopez, Adam and Karakos, Damianos and Callison-Burch, Chris and Khudanpur, Sanjeev},
  journal={LDC2014T23. Web Download. Philadelphia: Linguistic Data Consortium},
  year={2014}
}",\cite hkust,"@article{jsut,
  title={JSUT corpus: free large-scale Japanese speech corpus for end-to-end speech synthesis},
  author={Sonobe, Ryosuke and Takamichi, Shinnosuke and Saruwatari, Hiroshi},
  journal={arXiv preprint arXiv:1711.00354},
  year={2017}
}","@inproceedings{libri,
  title={Librispeech: an asr corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5206--5210},
  year={2015},
  organization={IEEE}
}","@article{reverb1,
  title={A summary of the REVERB challenge: state-of-the-art and remaining challenges in reverberant speech processing research},
  author={Kinoshita, Keisuke and Delcroix, Marc and Gannot, Sharon and P. Habets, Emanu{\""e}l A and Haeb-Umbach, Reinhold and Kellermann, Walter and Leutnant, Volker and Maas, Roland and Nakatani, Tomohiro and Raj, Bhiksha and others},
  journal={EURASIP Journal on Advances in Signal Processing},
  volume={2016},
  pages={1--19},
  year={2016},
  publisher={Springer}
}","@article{switchboard21,
  title={Switchboard-1 Release 2},
  author={Godfrey, John J and Holliman, Edward},
  journal={Linguistic Data Consortium, Philadelphia},
  volume={926},
  pages={927},
  year={1997}
}","@misc{ted21,
        author = {A. Rousseau, P. Deléglise, and Y. Estève},
        title = {TED-LIUMv2},
        howpublished = {\url{https://openslr.magicdatatech.com/19/}},
        year = {2014},
        note = {[Accessed 25-Jun-2023]},
}","@inproceedings{ted3,
  title={TED-LIUM 3: Twice as much data and corpus repartition for experiments on speaker adaptation},
  author={Hernandez, Fran{\c{c}}ois and Nguyen, Vincent and Ghannay, Sahar and Tomashenko, Natalia and Esteve, Yannick},
  booktitle={Speech and Computer: 20th International Conference, SPECOM 2018, Leipzig, Germany, September 18--22, 2018, Proceedings 20},
  pages={198--208},
  year={2018},
  organization={Springer}
}","@misc{vox1,
        author = {Ken MacLean},
        title = {{F}ree {S}peech... {R}ecognition ({L}inux, {W}indows and {M}ac) - voxforge.org},
        howpublished = {\url{https://www.voxforge.org/}},
        year = {},
        note = {[Accessed 25-Jun-2023]},
}",\cite{wsj23},\citewsj0,"@misc{aishell12,
  author       = ""Beijing Shell Shell Technology Co., Ltd"",
  title        = ""Open Source Mandarin Speech Corpus
[AISHELL-ASR0009-OS1]
Training and Test Data "",
  howpublished = ""online"",
  month        = ""Nov"",
  year         = ""2018"",
  note         = ""documentation"",

}","@inproceedings{kaldiwsj0,
  title={The Kaldi speech recognition toolkit},
  author={Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and Schwarz, Petr and others},
  booktitle={IEEE 2011 workshop on automatic speech recognition and understanding},
  number={CONF},
  year={2011},
  organization={IEEE Signal Processing Society}
}","@misc{kaldihkust,
        author = {ma08},
        title = {Kaldi HKUST},
        howpublished = {\url{https://github.com/kaldi-asr/kaldi/tree/master/egs/hkust}},
        year = {2016},
        note = {[Accessed 25-Jun-2023]},
}","@article{hkustSpeech1,
  title={Hkust mandarin telephone speech, part 1},
  author={Fung, P and Huang, S and Graff, D},
  journal={LDC2005S15. Web download},
  year={2005}
}",\citeHKUST,"@misc{fishtrans,
        author = {David Graff, Shudong Huang, Ingrid Cartagena, Kevin Walker, Christopher Cieri},
        title = {Fisher Spanish - Transcripts},
        howpublished = {\url{https://catalog.ldc.upenn.edu/LDC2010T04}},
        year = {2010},
        note = {[Accessed 25-Jun-2023]},
}","@misc{fishspeech,
        author = {David Graff, Shudong Huang, Ingrid Cartagena, Kevin Walker, Christopher Cieri},
        title = {{F}isher {S}panish {S}peech - {L}inguistic {D}ata {C}onsortium --- catalog.ldc.upenn.edu},
        howpublished = {\url{https://catalog.ldc.upenn.edu/LDC2010S01}},
        year = {2010},
        note = {[Accessed 25-Jun-2023]},
}",\amt,"@misc{calltrans1,
        author = {Barbara Wheatley},
        title = {{C}{A}{L}{L}{H}{O}{M}{E} {S}panish {T}ranscripts - {L}inguistic {D}ata {C}onsortium --- catalog.ldc.upenn.edu},
        howpublished = {\url{https://catalog.ldc.upenn.edu/LDC96T17}},
        year = {1997},
        note = {[Accessed 25-Jun-2023]},
}",,"@misc{librivox,
        author = {Hugh McGuire},
        title = { {L}ibri{V}ox | free public domain audiobooks  --- librivox.org},
        howpublished = {\url{https://librivox.org/}},
        year = {2005},
        note = {[Accessed 25-Jun-2023]},
}","@inproceedings{wsjcam0,
  title={WSJCAMO: a British English speech corpus for large vocabulary continuous speech recognition},
  author={Robinson, Tony and Fransen, Jeroen and Pye, David and Foote, Jonathan and Renals, Steve},
  booktitle={1995 International Conference on Acoustics, Speech, and Signal Processing},
  volume={1},
  pages={81--84},
  year={1995},
  organization={IEEE}
}","@inproceedings{mcwsjav1,
  title={The multi-channel Wall Street Journal audio visual corpus (MC-WSJ-AV): Specification and initial experiments},
  author={Lincoln, Mike and McCowan, Iain and Vepa, Jithendra and Maganti, Hari Krishna},
  booktitle={IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.},
  pages={357--362},
  year={2005},
  organization={IEEE}
}","@inproceedings{switchboard1,
  title={SWITCHBOARD: Telephone speech corpus for research and development},
  author={Godfrey, John J and Holliman, Edward C and McDaniel, Jane},
  booktitle={Acoustics, Speech, and Signal Processing, IEEE International Conference on},
  volume={1},
  pages={517--520},
  year={1992},
  organization={IEEE Computer Society}
}","@misc{ted,
        author = {TED Conferences LLC.},
        title = {translate transcribe --- ted.com},
        howpublished = {\url{https://www.ted.com/participate/translate/transcribe}},
        year = {2022},
        note = {[Accessed 25-Jun-2023]},
}",pap4,"@misc{ljspeech,
  author       = {Keith Ito and Linda Johnson},
  title        = {The LJ Speech Dataset},
  howpublished = {\url{https://keithito.com/LJ-Speech-Dataset/}},
  year         = 2017
}","@misc{Lindalibrivox,
        author = {LibriVox, Linda Johnsonn},
        title = {Linda Johnsonn {L}ibri{V}ox --- librivox.org},
        howpublished = {\url{https://librivox.org/sections/readers/11049}},
        year = {2016},
        note = {[Accessed 25-Jun-2023]},
}",\amt,pap5,"@article{asv1,
  title={ASVspoof 2019: A large-scale public database of synthesized, converted and replayed speech},
  author={Wang, Xin and Yamagishi, Junichi and Todisco, Massimiliano and Delgado, H{\'e}ctor and Nautsch, Andreas and Evans, Nicholas and Sahidullah, Md and Vestman, Ville and Kinnunen, Tomi and Lee, Kong Aik and others},
  journal={Computer Speech \& Language},
  volume={64},
  pages={101114},
  year={2020},
  publisher={Elsevier}
}","@inproceedings{cstr1,
  title={The voice bank corpus: Design, collection and data analysis of a large regional accent speech database},
  author={Veaux, Christophe and Yamagishi, Junichi and King, Simon},
  booktitle={2013 international conference oriental COCOSDA held jointly with 2013 conference on Asian spoken language research and evaluation (O-COCOSDA/CASLRE)},
  pages={1--4},
  year={2013},
  organization={IEEE}
}","@inproceedings{tacotron,
  title={Natural tts synthesis by conditioning wavenet on mel spectrogram predictions},
  author={Shen, Jonathan and Pang, Ruoming and Weiss, Ron J and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and others},
  booktitle={2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={4779--4783},
  year={2018},
  organization={IEEE}
}","@article{wavenet1,
  title={Wavenet: A generative model for raw audio},
  author={Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1609.03499},
  year={2016}
}","@article{deepvoice,
  title={Deep voice 3: 2000-speaker neural text-to-speech},
  author={Ping, Wei and Peng, Kainan and Gibiansky, Andrew and Arik, Sercan O and Kannan, Ajay and Narang, Sharan and Raiman, Jonathan and Miller, John},
  journal={proc. ICLR},
  pages={214--217},
  year={2018}
}",\cite prev,\citeother,"@article{darpatimit,
  title={DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1},
  author={Garofolo, John S and Lamel, Lori F and Fisher, William M and Fiscus, Jonathan G and Pallett, David S},
  journal={NASA STI/Recon technical report n},
  volume={93},
  pages={27403},
  year={1993}
}","@misc{magnatag1,
        author = {Edith Law },
        title = {{P}apers with {C}ode - {M}agna{T}ag{A}{T}une {D}ataset --- paperswithcode.com},
        howpublished = {\url{https://paperswithcode.com/dataset/magnatagatune}},
        year = {2019},
        note = {[Accessed 25-Jun-2023]},
}","@inproceedings{tagatune,
  title={Input-agreement: a new mechanism for collecting data using human computation games},
  author={Law, Edith and Von Ahn, Luis},
  booktitle={Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages={1197--1206},
  year={2009}
}","@misc{magnatune,
        author = {John Buckman},
        title = {{I}nformation: about {M}agnatune --- magnatune.com},
        howpublished = {\url{http://magnatune.com/info/}},
        year = {2003},
        note = {[Accessed 25-Jun-2023]},
}","@misc{freesound,
	author = {the Music Technology Group of Universitat Pompeu Fabra},
	title = {{F}reesound --- freesound.org},
	howpublished = {\url{https://freesound.org/}},
	year = {2005},
	note = {[Accessed 25-Jun-2023]},
}"
Another place I found the resource,,,"@INPROCEEDINGS{imagenet2,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  doi={10.1109/CVPR.2009.5206848}}",,\cite{imagenet1} \cite{imagenet2},,,,,,"@misc{datatang2,
        author = {},
        title = {{P}apers with {C}ode - aidatatang_200zh {D}ataset --- paperswithcode.com},
        howpublished = {\url{https://paperswithcode.com/dataset/aidatatang-200zh}},
        year = {},
        note = {[Accessed 25-Jun-2023]},
}",,,,"@book{csj2,
   author         = ""国立国語研究所"",
   title         = ""日本語話し言葉コーパスの構築法"",
   publisher         = ""国立国語研究所"",
   year          = ""2006"",
   month         = ""mar""
}",,"@inproceedings{fishhome2,
      Title = {Improved Speech-to-Text Translation with the {F}isher and {C}allhome {S}panish--{E}nglish Speech Translation Corpus},
      Author = {Post, Matt and Kumar, Gaurav and Lopez, Adam and Karakos, Damianos and Callison-Burch, Chris and Khudanpur, Sanjeev},
      Booktitle = {Proceedings of the International Workshop on Spoken Language Translation (IWSLT)},
      Year = {2013},
      Address = {Heidelberg, Germany},
      Month = {December}
    }","@misc{hkust2,
        author = {Hong Kong University of Science and Technology},
        title = {{L}inguistic {D}ata {C}onsortium {C}atalog},
        howpublished = {\url{https://catalog.ldc.upenn.edu/docs/LDC2005S15/}},
        year = {2007},
        note = {[Accessed 25-Jun-2023]},
}",,,"@misc{reverb2,
        author = {Keisuke Kinoshita, Marc Delcroix, Takuya Yoshioka, Tomohiro Nakatani},
        title = {{T}he {R}{E}{V}{E}{R}{B} challenge - {E}valuating de-reverberation and {A}{S}{R} techniques in reverberant environments --- reverb2014.dereverberation.com},
        howpublished = {\url{https://reverb2014.dereverberation.com}},
        year = {2014},
        note = {[Accessed 25-Jun-2023]},
}","@misc{switchboard22,
        author = {John J. Godfrey, Edward Holliman},
        title = {{S}witchboard-1 {R}elease 2 - {L}inguistic {D}ata {C}onsortium --- catalog.ldc.upenn.edu},
        howpublished = {\url{https://catalog.ldc.upenn.edu/LDC97S62}},
        year = {1997},
        note = {[Accessed 25-Jun-2023]},
}","@inproceedings{ted22,
  title={Enhancing the TED-LIUM corpus with selected data for language modeling and more TED talks.},
  author={Rousseau, Anthony and Del{\'e}glise, Paul and Esteve, Yannick and others},
  booktitle={LREC},
  pages={3935--3939},
  year={2014}
}",,"@misc{vox2,
        author = {VoxForge},
        title = {{P}apers with {C}ode - {V}ox{F}orge {D}ataset},
        howpublished = {\url{https://paperswithcode.com/dataset/voxforge}},
        year = {2019},
        note = {[Accessed 25-Jun-2023]},
}",,,,,,"@misc{hkustSpeech2,
	author = {Pascale Fung, Shudong Huang, David Graff},
	title = {Mandarin Chinese Conversational Telephone Speech & Transcripts, PART 1},
	howpublished = {\url{https://catalog.ldc.upenn.edu/docs/LDC2005S15/}},
	year = {2005},
	note = {[Accessed 25-Jun-2023]},
}",,,,,"@misc{calltrans2,
        author = {Barbara Wheatley},
        title = {{C}{A}{L}{L}{H}{O}{M}{E} {S}panish {T}ranscripts - {L}inguistic {D}ata {C}onsortium --- catalog.ldc.upenn.edu},
        howpublished = {\url{https://catalog.ldc.upenn.edu/docs/LDC96T17/}},
        year = {1997},
        note = {[Accessed 25-Jun-2023]},
}",\cite calltrans2,,,"@misc{mcwsjav2,
        author = {Erich Zwyssig, Mike Lincoln},
        title = {MC-WSJ-AV corpus --- catalog.ldc.upenn.edu},
        howpublished = {\url{https://catalog.ldc.upenn.edu/docs/LDC2014S03/README.pdf}},
        year = {2012},
        note = {[Accessed 25-Jun-2023]},
}",,,,,,,,"@misc{asv2,
        author = {Junichi Yamagishi},
        title = { | {A}{S}{V}spoof 2019 --- asvspoof.org},
        howpublished = {\url{https://www.asvspoof.org/index2019.html}},
        year = {2019},
        note = {[Accessed 25-Jun-2023]},
}","@misc{cstr2,
        author = {Junichi Yamagishi},
        title = { CSTR VCTK Corpus 
      English Multi-speaker Corpus for CSTR Voice Cloning Toolkit },
        howpublished = {\url{https://datashare.ed.ac.uk/bitstream/handle/10283/3443/README.txt?sequence=1&isAllowed=y}},
        year = {2019},
        note = {[Accessed 25-Jun-2023]},
}",,"@misc{wavenet2,
        author = {Aäron van den Oord, Sander Dieleman},
        title = {{W}ave{N}et: {A} generative model for raw audio --- deepmind.com},
        howpublished = {\url{https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio}},
        year = {2016},
        note = {[Accessed 25-Jun-2023]},
}",,,,,"@inproceedings{magnatag2,
  title={Evaluation of algorithms using games: The case of music tagging.},
  author={Law, Edith and West, Kris and Mandel, Michael I and Bay, Mert and Downie, J Stephen},
  booktitle={ISMIR},
  pages={387--392},
  year={2009},
  organization={Citeseer}
}",,,
Another place I found the resource,,,,,,,,,,,"@misc{datatang3,
        author = {speechocean},
        title = {{T}he largest open source {C}hinese corpus and another five speech recognition datasets --- en.speechocean.com},
        howpublished = {\url{https://en.speechocean.com/Cy/778.html}},
        year = {},
        note = {[Accessed 25-Jun-2023]},
}",,,,,,"@misc{fishhome3,
        author = {    Post, Matt and Kumar, Gaurav and Lopez, Adam and Karakos, Damianos and Callison-Burch, Chris and Khudanpur, Sanjeev},
        title = {{L}inguistic {D}ata {C}onsortium {C}atalog --- catalog.ldc.upenn.edu},
        howpublished = {\url{https://catalog.ldc.upenn.edu/docs/LDC2014T23/}},
        year = {},
        note = {[Accessed 25-Jun-2023]},
}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"@article{asv3,
  title={ASVspoof 2019: spoofing countermeasures for the detection of synthesized, converted and replayed speech},
  author={Nautsch, Andreas and Wang, Xin and Evans, Nicholas and Kinnunen, Tomi H and Vestman, Ville and Todisco, Massimiliano and Delgado, H{\'e}ctor and Sahidullah, Md and Yamagishi, Junichi and Lee, Kong Aik},
  journal={IEEE Transactions on Biometrics, Behavior, and Identity Science},
  volume={3},
  number={2},
  pages={252--265},
  year={2021},
  publisher={IEEE}
}",,,,,,,,"@misc{magnatag3,
        author = {Edith Law, Kris West, Michael Mandel, Mert Bay and J. Stephen Downie},
        title = {{T}he {M}agna{T}ag{A}{T}une {D}ataset | {C}ity {U}niversity {M}{I}{R}{G} --- mirg.city.ac.uk},
        howpublished = {\url{https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset}},
        year = {2009},
        note = {[Accessed 25-Jun-2023]},
}",,,
Another place I found the resource,,,,,,,,,,,"@misc{openslrOpenslrorg,
        author = {Beijing DataTang Technology Co., Ltd},
        title = {aidatatang_200zh},
        howpublished = {\url{http://openslr.org/62/}},
        year = {},
        note = {[Accessed 25-Jun-2023]},
}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Title,SA-Net: Shuffle attention for deep convolutional neural networks,,,,,,Attention is all you need in speech separation,,,Recent developments on ESPNeT toolkit boosted by conformer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1-DOI main paper,10.1109/ICASSP39728.2021.9414568,,,,,,10.1109/ICASSP39728.2021.9413901,,,10.1109/ICASSP39728.2021.9414858,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.1109/ICASSP39728.2021.9413889,,,,10.1109/ICASSP39728.2021.9414234,,,,,,,,,,,,
1-link/doi current,,ResNet50: https://nl.mathworks.com/help/deeplearning/ref/resnet50.html,ImageNet: https://huggingface.co/datasets/imagenet-1k (provides info on the annotation precess),MS COCO: https://doi.org/10.48550/arXiv.1405.0312,Amazon Mechanical Turk: https://www.mturk.com/,WordNet: https://wordnetcode.princeton.edu/5papers.pdf (info on creation),,WSJ0-2/3mix: https://arxiv.org/pdf/1508.04306.pdf ,WSJ0: https://aclanthology.org/H92-1075.pdf ,,AIDATATANG: https://www.datatang.ai/annotation ,AISHELL-1: https://arxiv.org/pdf/1709.05522.pdf AND https://catalog.ldc.upenn.edu/docs/LDC2018S14/AISHELL-1_specification.pdf ,AISHELL-2: https://arxiv.org/pdf/1808.10583v2.pdf,AURORA4: http://dnt.kr.hsnr.de/aurora/download/report_largevocab_v21.pdf,CSJ: https://clrd.ninjal.ac.jp/csj/en/document.html AND https://repository.ninjal.ac.jp/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=1373&item_no=1&page_id=13&block_id=21,CHiME4: https://spandh.dcs.shef.ac.uk/chime_challenge/CHiME4/,Fisher-CallHome: https://catalog.ldc.upenn.edu/LDC2014T23 AND https://catalog.ldc.upenn.edu/docs/LDC2014T23/,HKUST: https://catalog.ldc.upenn.edu/LDC2005T32 AND https://catalog.ldc.upenn.edu/docs/LDC2005S15/README.txt,JSUT:https://sites.google.com/site/shinnosuketakamichi/publication/jsut AND https://arxiv.org/pdf/1711.00354.pdf,LibriSpeech: http://www.danielpovey.com/files/2015_icassp_librispeech.pdf,REVERB: https://reverb2014.dereverberation.com/index.html AND https://reverb2014.dereverberation.com/data.html,SwitchBoard: 2015 release https://catalog.ldc.upenn.edu/LDC97S62,TEDLIUM2: https://openslr.magicdatatech.com/19/,TEDLIUM3:https://arxiv.org/pdf/1805.04699.pdf AND https://www.openslr.org/51/,VoxForge,WSJ0-2/3mix: https://arxiv.org/pdf/1508.04306.pdf ,WSJ0: https://aclanthology.org/H92-1075.pdf ,AISHELL-ASR0009: http://aishell-doc.oss-cn-beijing.aliyuncs.com/AISHELL-ASR0009.pdf,Kaldi framwork for WSJ0,Kaldi HKUST recipe: https://github.com/kaldi-asr/kaldi/tree/master/egs/hkust,HKUST Mandarin Telephone Speech https://catalog.ldc.upenn.edu/LDC2005S15 AND https://catalog.ldc.upenn.edu/docs/LDC2005S15/README.txt,HKUST Mandarin Telephone Transcript Data: https://catalog.ldc.upenn.edu/LDC2005T32  AND https://catalog.ldc.upenn.edu/docs/LDC2005S15/README.txt,LDC Fisher Spanish - Transcripts: https://catalog.ldc.upenn.edu/LDC2010T04,LDC Fisher Spanish Speech: https://catalog.ldc.upenn.edu/LDC2010S01,AMT for translating from Spanish to English,The CALLHOME Spanish Transcripts : https://catalog.ldc.upenn.edu/LDC96T17 AND https://catalog.ldc.upenn.edu/docs/LDC96T17/  AND for detailed=> https://catalog.ldc.upenn.edu/docs/LDC96T17/ch_span.txt,The CALLHOME Spanish Speech: https://catalog.ldc.upenn.edu/docs/LDC96T17/ch_span.txt,LibriVox,WSJCAM0: A British English speech corpus for large vocabulary continuous speech recognition : https://www.academia.edu/47336761/WSJCAMO_A_British_English_speech_corpus_for_large_vocabulary_continuous_speech_recognition (I could not reach it because TU Delft was not in the list) AND https://www.academia.edu/47336761/WSJCAMO_A_British_English_speech_corpus_for_large_vocabulary_continuous_speech_recognition (I made an account here and reached it),MC-WSJ-AV: https://catalog.ldc.upenn.edu/docs/LDC2014S03/README.pdf,"SWITCHBOARD: telephone speech corpus for research and development
https://isip.piconepress.com/projects/switchboard/doc/education/papers/paper_1.pdf",TED Talks: https://www.ted.com/participate/translate/transcribe,FastPitch: Parallel text-to-speech with pitch prediction,LJSpeech: https://keithito.com/LJ-Speech-Dataset/,LibriVox data of Linda Johnson: https://librivox.org/reader/11049?primary_key=11049&search_category=reader&search_page=1&search_form=get_results,AMT for evaluating the fastspeech algorithm,,ASVspoof2019,CSTR VCTK Corpus English Multi-speaker Corpus for CSTR Voice Cloning Toolkit: https://datashare.ed.ac.uk/bitstream/handle/10283/2651/README.txt?sequence=1&isAllowed=y AND https://sci-hub.se/10.1109/icsda.2013.6709856,Tacotron 2,WaveNet,Deep Voice 3,LibriSpeech: http://www.danielpovey.com/files/2015_icassp_librispeech.pdf,LibriVox,DARPA TIMIT: https://nvlpubs.nist.gov/nistpubs/Legacy/IR/nistir4930.pdf,"MagnaTagATune dataset: https://paperswithcode.com/dataset/magnatagatune
https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=8a1384e041cc6ea2735b01c734aeef666dc92884
https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset",Tag A Tune : https://dl.acm.org/doi/abs/10.1145/1518701.1518881?casa_token=LJuor_I0mNMAAAAA:oe2JBf2vuMA_rcuEHeeGvay27PYnY-n5_1YwPa6SRHnCfW7e9v_wfzEfdIvEK6B1Zj8oOhGs-l0V,MagnaTune Dataset: http://magnatune.com/info/,FreeSound:https://freesound.org/help/about/
1-data coming from,"Uses ResNet, ImageNet1k and COCO",Uses ImageNet as the only source of data,"Uses Flickr and search engines to gather images. The labels of those images (the search terms in Flickr and search engines) are determined with WordNet. For the first data collection, the creators also labelled the images. Later, they used AMT to check the validity of the labels.","Uses AMT for all the labelling, the images are ""from publicly available image repositories, online platforms, and contributions from individual photographers"".",Utilized to check validity of ImageNet,Data collected by lexicographers in several different universities.,data from WSJo-2/3mix,Data mixed from WSJ0,Random MIT academics reading the sentences.,trained on 17 datasets:,they collect and label raw speech data themselves.,they filtered and changed some datapoints from AISHELL-ASR0009. The filtering was done with Kaldi HKUST recipe (it is an algorithm but they originally call it a 'recipe') which is pre-trained by HKUST dataset.,"original data, the idea is the same as original AISHELL",uses WSJ0 data,"original data, they collected in a conference. ",WSJ0 and Kaldi framework,They create a new dataset by combining Fisher Spanish Translations dataset with CALLHOME Spanish dataset and then translating it using AMT.,uses HKUST Mandarin Telephone Speech,"text source is not given, it contains one woman's 10 hour voice recordings",audio data from LibriVox,comes from two datasets: MC-WSJ-AV and WSJCAM0 ,comes from SWITCHBOARD original,from ted talks,from ted talks and TED-LIUMv2,comes from contributors,Data mixed from WSJ0,Random MIT academics reading the sentences.,"they are the original source, 2000 speakers with different accents",uses WSJ0 data. ,comes from HKUST,They create the data through chatting,uses HKUST Mandarin Telephone Speech,from LDC Fisher Spanish Speech,original dataset,from the combined dataset containing Fisher Spanish Transcript and CALLHOME Spanish Transcript datasets.,they transcribed the data of The CALLHOME Spanish Speech,they collected the data,,WSJ0 text,edits and filters WSJCAM0,original,ted talks,takes one female reader data from LJSpeech which takes its data from the audio book website librivox.,Linda Johnson's audio book recordings that is submitted to LibriVox,,the data given by fastspeech. Their generated speech audio,,CSTR VCTK corpus,original,comes from Deep Voice 3,"comes from Google's Mandarin Chinese TTS dataset, a YouTube piano dataset, MagnaTagATune and DARPA TIMIT
Google’s North American English TTS dataset and MagnaTagATune","CSTR VCTK Corpus and
LibriSpeech ASR corpus",audio data from LibriVox,,original dataset creators,comes from tag a tune game which was pretrained by both MagnaTune label's songs and the sounds from FreeSound library,comes from MagnaTune song dataset and FreeSound ,their musicians' data,their users' audio data
original name (if it is written wrongly),,,,,,,,,,,Datatang ,,,Aurora-4,,4th CHiME,Fisher and CALLHOME,HKUST Mandarin Telephone Transcript Data,"JSUT (Japanese speech corpus of Saruwatari-lab., University of Tokyo)",LibriSpeech ASR corpus,The REVERB (REverberant Voice Enhancement and Recognition Benchmark) ,"Switchboard-1 Release 2 They did not specify the version, they probably used the latest one since they published it on 2021-2022 and the latest release is in 2015. ",TED-LIUMv2,TED-LIUM Release 3,VoxForge English,,,,,,,,,,,,,,,,,,,,,,End-to-end anti-spoofing with rawnet2,,,,,,LibriSpeech ASR corpus,,,,,,
2-Quick notes,"Uses ResNet, ImageNet1k and MS COCO",ResNet50 - is a deep learning architecture known for its effectiveness in image classification tasks. They can use it pre-trained by ImageNet. ,"ImageNet-1k: popular dataset in computer vision for image classification tasks. 1k refers to a subset of ImageNet that contains 1,000 classes.","MS COCO: (Common Objects in Context) popular dataset used for various computer vision tasks, consists of a large collection of images with detailed annotations. Used AMT for all theh labelling.","AMT: is an online market place which enables individuals and businesses to outsource small tasks,","WordNet is a lexical database that organizes words into synsets (sets of synonyms) and provides structured relationships between words, facilitating semantic analysis and language understanding.","Uses WSJ0-2/3mix and they did not give any info related to the dataset used. They just said  ""hey we used this"" and there is no validation or reasoning why they decided using that instead of others.","Datasets created using WSJ0 dataset. 2 refers to mixing two different speeches, 3 refers mixing three different speeches. This is done to mimic 2-3 people talking. ",Created firstly on 1991. 60 people joined for the creation. They read parts from Wall Street Journal.,They used 17 datasets to train and test their system. They wrote half of the dataset names incorrectly and not give any info on annotation practices or any content related to the datasets utilized.,datatang is a company collecting and labelling raw data. Generally Chinese speech data.,"data filtered from AISHELL-ASR0009 a lot and used Kaldi HKUST algorithm which is a pre-trained language model by HKUST dataset. AISHELL-ASR0009 contained 2000 speakers, this one they filtered to contain 400 speakers. ",Chinese Mandarin speech dataset. They are repeating AISHELL original and AISHELL-1 with a greater amount of samples and with a greater amount of quality. Everything related to the data collection seems similar with original AISHELL.,"edits and filters  WSJ0 dataset to create a new dataset. They add traffic sound, baby crying sound and some other random sounds (bird, dog etc) on the background of WSJ0 dataset. their main website seems extremely old and unprofessional, they are using html (http://aurora.hsnr.de/aurora-4.html). It is probably because it is a dataset from 2002.","Japanese speech dataset, all of their documentation is Japanese I needed to use extensive usage of google translate. They received all the lecture recording audio, some random chats which happened during the conference, the lecturers' own works read outloud by them, re-reading the lecturers' lectures in transcribed version.","it is a dataset prepared for a competition. It mainly WSJ0 data and random noise data. In WSJ0 dataset, there are noisy real data. for the challenge, clean data from WSJ0 is mixed with random noises and the challenge is to differenciate which is artificially noisy data and which is real noisy data. WSJ0 is external dataset and the noise data source is not given, probably from Kaldi framework. To compete, the base implementation is given through Kaldi and it uses WSJ0.",They combined Fisher Spanish Transcriptions dataset and CALLHOME Spanish Transcript datasets and then translated the joined dataset with AMT. ,"There are 2 different datasets containing the name HKUST, but the other one does not contain labels thus, this should be the one they have utilized. Regular Chinese people trasncribing the HKUST Mandarin Telephone Speech dataset. ","Japanese dataset of one woman's voice recording for 10 hours with many different topics. It contains speech, song and audio. They do not say where their text comes from. They have 9 domains for the audio version but they did not specify where they get the text for those from.",Used LibriVox as the English audiobook website to gather data. 1000 hours of data.,It creates a combined dataset with two datasets: MC-WSJ-AV and WSJCAM0. MC-WSJ-AV is an filtered and edited version of WSJCAM0. ,Highly edited version of SWITCHBOARD original. No additional annotation is added,making a corpus by preparing and filtering ted talk content,uses TED-LIUMv2 data and adds the same size of another. LIUM joins Ubiqus company for this project. ,opensource dataset where random people upload random labelled audio,"Datasets created using WSJ0 dataset. 2 refers to mixing two different speeches, 3 refers mixing three different speeches. This is done to mimic 2-3 people talking. ",Created firstly on 1991. 60 people joined for the creation. They read parts from Wall Street Journal.,"original dataset,  total of 3002 hours speaking with more than 2000 speakers. The text contains relevant data from 17 domains (smart home, autonomous vehicle commands, industrial production, music science, finance etc). Very strict rules on the recording and checking. 97% accuracy with the read text. They deleted politically sensitive, personal privacy, pornographic violence and other content after the recording. They give every information possible in their documentation.",A pre-trained framework which uses WSJ0.,this is a pre-trained model of Kaldi. It is trained on HKUST dataset. Their HKUST dataset links did not work. They trained on it by using both the 'Speech' and 'Transcript' datasets of HKUST.,Regular Chinese people having regular chats of 10 minutes with the people they do not know. This dataset exists and it was just collected without any labels/texts associated with it. ,Regular Chinese people trasncribing the HKUST Mandarin Telephone Speech dataset. ,transcribed telephone conversations from LDC Fisher Spanish Speech. The Fisher Spanish data set consists of 819 transcribed conversations on an assortment of provided topics primarily between strangers. 160 hours of speech. The transcriptions are done by hiring students to transcribe the audio (this part was learned through email).,Made a platform where random people can talk about a random topic selected from a list of topics. The topic changes every day. Each conversation is 10-12 minutes.,they combined the datasets and put all the data to AMT to translate,This is the transcription of The CALLHOME Spanish Speech data. ,"The CALLHOME Spanish corpus comprises 120 transcripts of spontaneous conversations primarily between friends and family members, resulting in approximately 20 hours of speech aligned at the utterance level, with just over 200,000 words (tokens) of transcribed text. The people selected are the aquiantices of the researchers and people called their family and friends. They were supposed to talk at max 30 minutes. The phone calls were done through internet and recorded. via a toll-free robot operator maintained originally by Rutgers University, and later by the LDC.",English audiobook website,Uses WSJ0 text and reads it in English accent. Copy of WSJ0 in English accent rather than American accent. Created 1995.,edits and filters WSJCAM0,"original dataset, DARPA sponsored it and Texas Instruments are utilized to autorecord it. They trained and tested for usabiity and correctness for a long time. DARPA hired transcriber even though they did not specify. Random people calling random other people in the system.",volunteer transcribers transcribe the talks,trained on LJSpeech dataset and tested their system with AMT,they take librivox data from 20216-2017 of Linda Johnson and all the aligning is done by Keith Ito.,English audiobook website,they used AMT to evaluatie their algorthm. They did not give much info on it so I do n ot how they did teir tests.,only uses AVSSpoof2019 dataset,uses CSTR VCTK corpus with two pre trained models: WaveNet and Tacotron,very transparent and open,"they are a pre-trained model, their data comes from deep voice 3","It is also a pretrained model, they use several sources but I was only able to MagnaTagATune and DARPA TIMIT","This is also a model which was trained on CSTR VCTK Corpus and
LibriSpeech ASR corpus",Used LibriVox as the English audiobook website to gather data. 1000 hours of data.,English audiobook website,1990 dataset which covers 8 major dialects/accents of American English language. Their dataset used to be distriuted as a CD. Their documentation seems very old as well but they tried to be very open and representative in terms of tehir sampling and their images. I loved the America map and their colorful pins that represent the accents.,this is the dataset created by playing tag a tune game with magnatune songs and songs from freesounf library. ,it is a game which was played with MagnaTune label's song dataset and FreeSound,"it is a music label providing song audio. They are like an ""opensource music label""",it is a open sound library providing random audio. 
3-Type,Literature,P,Combined,Combined,modified,Initial,Literature,modified,Initial,Literature,Initial,C,Initial,modified,Initial,C,C,Initial,Initial,Modified,Combined,modified,modified,C,Initial,modified,Initial,Initial,Pre-trained Model,Pre-trained Model,Initial,Initial,Initial,Initial,modified,Initial,Initial,Initial,modified,modified,Initial,Initial,Literature,Modified,Initial,Modified,Literature,Combined,Initial,Pre-trained Model,Pre-trained Model,Pre-trained Model,Modified,Initial,Initial,Modified,pre-trained model,Initial,Initial
4-stating version,Implicit,Explicit,Explicit,Implicit,Explicit,Implicit,Explicit,Explicit,Explicit,Implicit,Explicit,Implicit,Explicit,Implicit,Explicit,Implicit,Explicit,Explicit,Explicit,Implicit,Implicit,Implicit,Explicit,Explicit,Explicit,Explicit,Explicit,Explicit,Implicit,Implicit,Explicit,Explicit,Explicit,Explicit,Implicit,Explicit,Explicit,Explicit,Implicit,Implicit,Explicit,Explicit,Implicit,Implicit,Implicit,Explicit,Implicit,Implicit,Explicit,Implicit,Implicit,Explicit,Implicit,Explicit,Explicit,Implicit,Implicit,Implicit,Explicit
5-Optional additional comments,,,,did not inform due to copyright I assume,,,,,,have inconsistencies with their datasets. More than one of their datasets uses WSJ0 and they did not mention that.,,,,,"all documentation is japanese, i dont know japanese",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q1:was the work an original task?,no,no,"yes, it was one of the first big image datasets","kind of yes, the labelling was done very meticulously",-,yes,no,"Not sure, they tell that they did something groundbreaking but it is just copy pasting from the previous dataset WSJ0.","Yes, i think so",no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,"Not sure, they tell that they did something groundbreaking but it is just copy pasting from the previous dataset WSJ0.","Yes, i think so",no,no,no,no,no,no,no,no,no,no,"no, they are an audiobook website",no,no,no,no,no,no,"no, they are an audiobook website",no,no,no,no,no,no,no,no,"no, they are an audiobook website",no,no,no,no,no
Q2: did they use human annotations as labels?,yes,yes,yes,yes,-,yes,yes,Yes,Yes,yes,yes,yes,yes,yes,yes,yes,yes,yes,yes,"yes, there is the book and the person reading it",yes,yes,yes,yes,yes,Yes,Yes,yes,yes,yes,"no, there were no labels",yes,yes,"no, they do not have any annotation",yes,yes,there is no labels,there are people who wrote the stories and there are voice actresses reading the stories.,yes,yes,yes,yes,"yes, the people who prepared LJSpeech",yes,there are people who wrote the stories and for LJSpeech only Linda Johnson's voice was taken,"no, the labels were automatically generated by the algorithm",yes,yes,yes,"yes, their initial datasets (CSTR VCTK and LibriSpeech uses human labels)",yes in MagnaTagATune and DARPA TIMIT,yes,"yes, there is the book and the person reading it",there are people who wrote the stories and there are voice actresses reading the stories.,"yes, they had specific text where the speakers needed to read",yes,"yes, the players are the annotators",yes their musicians also label their own songs with genre and title,yes their users label their own audio with tags
Q3:did they use original human annotations? (i.e. annotations they collected themselves?),no,no,no,no,yes,yes,no,"No, they combine several of the previous data.",Yes,no,yes,no,yes,"no, they filter and add some extra noise on top of their external dataset",yes,no they edited the audio,no,"yes, they labeled the original data",yes,no,"no, they combined one original and one edited dataset","no, they filtered and edited it","no, they prepared and filtered ted talk data","no, they prepared and filtered ted talk data and combined it with TED-LIUMv2",yes,"No, they combine several of the previous data.",Yes,yes,no,yes,they collected the data,"yes, they labeled the original data",yes,"no, they do not have any annotation",no,yes,there is no labels,yes,yes,"no, they edited and filtered WSJCAM0",yes,yes,"yes, they did not edit it",no they edited according to the alignment of the speech,yes,no,no,no,yes,no,"no, MagnaTagATune gathers and labels the data through a game. The more people play it the better. For DARPA TIMIT, they are original dataset",no,no,yes,"yes, the texts",no,"no because if i understand correctly, more than one player can try to label the same audio",yes by their musicians labelling their own songs with genre and title,yes by their users labelling their own audio with tags
Q4:did they use external human annotations? (i.e. annotations from an existing dataset?) note: it might that they used both their original annotations as well as external annotations,yes,yes from ImageNet,"yes from Flickr, search engines, AMT for checking the validity.","yes from search engines, photo sharing apps and individual photographers",no,yes from well-known dictionaries,yes from WSJ0-2/3mix,yes from WSJ0,"no, they collected themselves","yes, all 17 of them",no,yes from AISHELL-ASR0009 and HKUST,no,yes from WSJ0,no,yes from Kaldi framework which uses WSJ0,"yes, the original ata came from Fisher Spanish Transcript and CALLHOME Spanish Transcript and they gave this joined dataset to be translated by AMT and used the data they get from AMT.",yes from HKUST Mandarin Telephone Speech,no,yes from VibriVox,yes from those two datasets,no,yes from ted talks,yes from ted talks and TED-LIUMv2,"yes, people uploaded their own data/datasets in the format that is necessary",yes from WSJ0,"no, they collected themselves",no,"yes, WSJ0",yes from HKUST Mandarin Telephone Speech and HKUST Mandarin Telephone Transcript Data,no,yes from HKUST Mandarin Telephone Speech,no,no,"yes, Fisher Spanish Transcript and CALLHOME Spanish Transcript",no,no,"not sure, the writers wrote their books",no they collected themselves,"yes, from WSJCAM0",no,no,"yes from LJSpeech, the voice actors' audio and the text data.",yes the book text is externald,"not sure, the writers wrote their books",yes from JLSpeech and their algorithm generating the text as well.,yes from ASVspoof 2019,yes from CSTR VCTK corpus,no,"yes, from CSTR VCTK and LibriSpeech","yes from Google's Mandarin Chinese TTS dataset, a YouTube piano dataset,  MagnaTagATune and DARPA TIMIT
Google’s North American English TTS dataset and MagnaTagATune","yes from CSTR VCTK Corpus and
LibriSpeech ASR corpus",yes from VibriVox,"not sure, the writers wrote their books",no,yes the data collected by tag a tune with MagnaTune's song dataset,no,no,no
"Q5:who were the annotators? (e.g. paper's authors, mturk, experts etc. - i.e. what population were they drawn from?)",Not Given -their data comes from those 3 datasets,Not given -all their data comes from imagenet,"the creators, the people who upload and tag their photos in Flickr and search engines, the AMT workers while filtering/checking.","AMT workers and the people uploading and tagging their own photos in search engines, photographers and in photo sharing apps",the AMT workers,the lexicographers,NG,they combined 2/3 datapoints from the external dataset. The annotators did not chance.,The people type the text of Wall Street Journal into the system. The readers were given texts from WSJ and the research team records everybody's reading and saving it with the text read. The readers are random MIT academics/students/workers.,NG ,the workers of datatang,the people who annotated the original sets.,"there were around 500 pre-written ""speeches"" and every reader read half an hour content. The gender ratio was balanced.","the editors were from the ""Aurora Group"", the original annotators of the data is WSJ0's annotators.",the people who received the training for Japanese linguistics,WSJ0 annotators and the people who create the training dataset.,"the original Spanish datasets, Fisher was transcribed by hired students, CALLHOME's is transcribed by 'transcribers funded by Texas Instruments'. The translation from Spanis to English was done through AMT.",the people hired by HK university. They mention it as 'regular people who can understand and differenciate between widely used Chinese accents'.,NG,no annotators but there are readers whor ead the book,no additional annotation was done for the combined dataset,"the annotators of the original SWITCHBOARD, no additional annotations done here","the volunteer transcribers of ted talks, no additional annotation is done here","the volunteer transcribers of ted talks, no additional annotation is done here",the contributors,they combined 2/3 datapoints from the external dataset. The annotators did not chance.,The people type the text of Wall Street Journal into the system. The readers were given texts from WSJ and the research team records everybody's reading and saving it with the text read. The readers are random MIT academics/students/workers.,the people choosing the texts for the people to read,NG,"NG here but, the people hired by HK university",nobody,the people hired by HK university. They mention it as 'regular people who can understand and differenciate between widely used Chinese accents'.,the hired students of University of Pennsylvania,no annotators but the people whose voices are recorded are mostly native Spanish speakers and are from USA and Puerto Rico. ,the AMT workers,they call it Texas Instrument's funded transcribers,there are no annotators but the speakers are the friends an family of the researchers,the writers of the books that were read by the voice actresses,"yes, the people who wrote the text of Wall Street Journal into the system. ","the annotators of WSJCAM0, no additional annotation was done.",the DARPA sponsored transcribers,the volunteer transcribers whose mother tingue is the spoken language in the talk,the algorithm that LJSpeech uses,the book writers and Keith Ito,the writers of the books that were read by Linda Johnson,JL Speech and the generated text of the algorithm,the people who annotated ASVspoof which are CSTR VCTK's annotators and the datasets of WaveNet's and Tcotron's annotators,the people of CSTR VCTK who are responsible for speakers' voices while they read teh texts,the speakers read the newspaper,the annotators of CSTR VCTK and LibriSpeech,the annotators of those inital datasets,the annotators of those corpuses,no annotators but there are readers whor ead the book,the writers of the books that were read by the voice actresses,"they had a person to record and give the instructions to the speaker along with the text. If the speaker reads the text wrongly, they would record again.",the people who play the vgame TagATune,the players,the musicians,the users
Q6:did they specify the number of annotators?,No,No,No,No,No,No,NG,No,No but it contains around 60 people's speeches.,NG ,no,no but the dataset contains the audio from 400 speakers and in total 170 hours of recording,no but there was 1991 people reading the sentences.,no,no,NG,no,no,no,no,no additional annotation,no additional annotations done here,no,no,NG,No,No but it contains around 60 people's speeches.,no but they specified that 2000 people's speeches were recorded. They also specify their age/gender/accent.,no,NG here but no,no,no,no,no but there were 136 people's voice recordings,no,no,there are no annotators ,no,"no, but there were 140 speakers",no additional annotation was done,no,no,no,"yes, 1",no,not applicable,no,no,no but they specified the # of speakers,no,no,no,no,no,no but they used 630 speakers,no,no,no,no
Q6a: did they estimate how many annotators they would need beforehand?,No,No,No,No,No,No,no,No,No,NG ,no,no,no but they wanted to have more people to have the recordings,"no, they did not use any annotator",no,NG,no,no,no,no,no additional annotation,no additional annotations done here,no,no,No,No,No,no,no,NG here but no,no,no,no,no,no,no,no,no,no,no additional annotation was done,no,no,no,no,no,not applicable,no,no,no,no,no,no,no,no,no,no,no,no,no
Q7: were there formal instructions for the annotators?,NG,Yes,"Yes and No -the AMT workers had, the people tagged their own photos no","Yes and No -yes for AMT workers, no for the people tagged their own photos",Yes,"yes, many pages of how to write a claim for a grouping",NG,No,"Yes, there were formal instructions for the people reading and recording. They specified the mic to be utilized, if there is a wrong reading, then the take starts from the start. The text is prepared beforehand and more than one person reads the same text.",NG ,yes,the included datapoints needed to have a certain quality and they need to contain equal amounts of both female and male datapoints. The Chinese needed to well spoken and the accents should cover most of the accents.,"for the recorders yes, they have the recording area requirements and how to add data to the dataset.",NG,"yes, there was a 500 page document and one section of it is on the instructions and the training of the annotators",NG,yes for all three of them,yes,no but there were formal instructions for the voice actress,there is not annotators but there are instructions for the voice actors,no additional annotation,no additional annotations done here,"no additional annotation is done in this stage, only preparing the audio data and filtering out ted talks","no additional annotation is done in this stage, only preparing the audio data and filtering out ted talks and combining them with and TED-LIUMv2",No,No,"Yes, there were formal instructions for the people reading and recording. They specified the mic to be utilized, if there is a wrong reading, then the take starts from the start. The text is prepared beforehand and more than one person reads the same text.","yes, they need to make sure that the speakers abide by the rukes written in their documentation.",NG,NG here but yes there were,not for the annotators but there was an 'operator' who made sure to call the right random people at the right time and initiate the conversation,yes,"no, only requirement was the students' mother lanuage needed to be Spanish","no annotators, but there were instructions for speakers",yes,yes,"there were no annotators, random people called the people they know and talked for 10-30 minutes",there were instructions for the voice actors,no but there were instructors for the readers,no additional annotation was done,"yes, they had a long pivot for usability and correctness for everyone.","yes, in the website the process of transcription is given.","for AMT yes, the speech data is taken from LJSpeech",ng,there were instructions for the voice actors in general,not applicable but there were formal insrtructions for AMT workers,NG,NG,no but there were for the speakers,NG,NG,NG,there is not annotators but there are instructions for the voice actors,there were instructions for the voice actors,no but there were instructioons for the recorders,there is how to play the game guideline,no but there are game instructions,not that I have seen,not that I have seen
Q8: was there training for the annotators?,No,NG,"Yes and No -the AMT workers had, the others no","Yes, they provided AMT workers with documents",There were instruction manuals with examples,"yes , their profession is related to that",NG,no,"yes, they made sure that the recorders record correctly and according to their manual.",NG ,yes,No,i assume so,NG,"yes, there was a 500 page document and one section of it is on the instructions and the training of the annotators",NG,NG,NG,no,there is not annotators but there are instructions for the voice actors,no additional annotation,no additional annotations done here,no additional annotation,no additional annotation,no,no,"yes, they made sure that the recorders record correctly and according to their manual.",no but no need to,NG,NG,no,NG,no,no,NG,NG,no annotators,no,"no and for the readers, they just hired random people in Cambridge University",no additional annotation was done,NG,no but no need to,"for AMT yes, the speech data is taken from LJSpeech",ng,no,not applicable but there were formal documents provided to AMT workers,NG,NG,no,NG,NG,NG,there is not annotators but there are instructions for the voice actors,no,not that i can find,there is how to play the game guideline,no but there are game instructions,no,no
Q9: was there prescreening on the crowdwork platforms?,NG,NG,"Yes and No -the AMT workers had, the others no",AMT does it automatically,Yes,they all needed to be lexicographers,NG,no,"they did not use any but for the readers, they tried to select equal amount of sexes and tried to be diverse in terms of the American accent (they tried to have people from different states).",NG but yes,they hire their workers,"yes, the people needed to know and be able to read Chinese properly, they tried to equalize the gender ratio ",all speakers needed to know and speak fluent in Chinese Mandarin,NG,no but they did not use it,NG,"yes, for AMT the workers need to know both languages as a bilingual proficency","no, they did not use any crowdwork platform but they had a prescreening for hiring the annotators.",no,Not relevant,no,no,no additional annotation,no additional annotation,it is crowdsourced but no prescreening,no,"they did not use any but for the readers, they tried to select equal amount of sexes and tried to be diverse in terms of the American accent (they tried to have people from different states).",no,NG,NG,no,"no, they did not use any crowdwork platform but they had a prescreening for hiring the annotators.",no,no external crowdsourcing but the speakers needed to fluently speak Spanish,yes the workers need to know both languages as a bilingual ability,NG,no,no,no,no,no,the only criteria is that the person is proficient in that language,yes for testing they used AMT,no,no,they used AMT to evaluate how good of a melodic speech their algortihm generates,they did not use crowd platforms,they did not use crowd platforms,no but they hired the speakers on a voluntary basis and on whether their mothertongue is British English,no,no,no,Not relevant,no,no,no,no,no,no
Q10: was there multiple annotator overlap? (i.e. did multiple annotators label the same item?),NG,NG,"Yes and No -the AMT workers yes, the others no","yes and no for AMT workers, no for the others",Yes,"when one claim is given, a lot of others need to agree so that they put in the database. They have a hyerarchical system.",NG,"no, their combination was fully random",yes,NG ,yes,No but the speakers needed to read a text and their accuracy was 97%,"Yes, they were reading the same documents",NG,"yes there were also data dublication, the lecturers sometimes repeated their lecture recordings by reading th etranscribed version of the lecture.",NG,NG,no,no,Not Relevant,no additional annotation,no additional annotations done here,no additional annotation,no additional annotation,no,"no, their combination was fully random",yes,"yes, the text was common for some of the speakers.",NG,NG,no,no,no,no,NG,NG,no,"i dont know, maybe there are more than one voice actor reading the same book",several people read the same text,no additional annotation was done,NG,"yes, they check it",i assume so,"no, keith ito aligned everything by himself","no, not for Linda Johnson's recordings",NG,NG,NG,yes several people read the same text,yes in both initial datasets,"for MagnaTagATune and DARPA TIMIT yes, the others no info was retireved","for CSTR VCTK yes, ",Not Relevant,"probably, famous bookswould be read more than one person",no but several people read the same text,I think so,"if i understand correctly, more than one player can try to label the same audio",no,no
Q11:reported inter-annotator agreement?,NG,NG,"Yes and No -the AMT workers had, the others no",yes between AMT,"Yes, based on the researcher it changes","yes, hyerarchic",NG,no,"no, but no need to",NG ,yes,No,no because the labels were already decided,NG,"yes, it is fully accordig to the document",NG,NG,no,no,Not Relevant,no additional annotation,no,no additional annotation,no additional annotation,no,no,"no, but no need to","not applicable, the text given is the ground truth",NG,NG,no,no,no,no,NG,NG,no,no,"no, but no need to",no,NG,"yes, the transcription is reviewed",ng,no,no,NG,NG,NG,no,NG,NG,NG,Not Relevant,no,no,NG,NG,no,no
Q11a: (or any other metric of label quality?),NG,NG,NG,NG,NG,their own metrics,NG,NG,NG,NG ,they have additional checks,they filtered by the quality of the audio and enhanced some features of the audio with current technologies before including them in the dataset.,"the quality of the audio, the gender ratio and representing different ages and different accents",NG,"the input and the label needs to be valid with regards to the 500 paged document. Only Tokyo accent is considered. Within the documentation, there are rules for labelling the abbreviations.",NG,NG,no,no,Not Relevant,no,no,no additional annotation,no additional annotation,no,NG,NG,they checked how much sentences are read correctly by the speakers. ,NG,NG,the Chinese spoken need to be a similar accent,no,no,no,NG,NG,no,no,NG,no,no,no,ng,no,no,NG,NG,NG,no,NG,NG,NG,Not Relevant,no,"if the text is read wrongly, the speaker needed to read it again",NG,NG,no,no
"Q12: did they link to the dataset? note: if they're using all external data, i.e. annotations and raw data, we'd like to track down the original reference and see if information on these questions is available","ImageNet, ResNet50, MS COCO","yes, to ImageNet","Flickr, the search engines and AMT for double checking/filtering","no, on purposely they did not","No, they were the initial dataset creator/filterer.","yes, https://wordnet.princeton.edu/","yes , WSJ0-2/3mix",yes to WSJ0,no,They put in a table by writing more than half of their names incorrectly.,no,yes to AISHELL-ASR0009 and the pretraiend model Kaldi HKUST recipe.,"no, they are original","yes, they used WSJ0.",no they are original.,they used Kaldi framework which uses WSJ0,"yes, Fisher Spanish Transcript and CALLHOME Spanish Transcript.","yes, HKUST Mandarin Telephone Speech",no,"Yes, they linked to the audio book website LibriVox",yes from those two datasets,yes SWITCHBOARD original,"yes, ted talks","yes, ted talks and TED-LIUMv2",no,yes to WSJ0,no,no,yes to WSJ0,"yes but their links did not work. From the name, I was able to reach them out. They are from HKUST Mandarin Telephone Speech and HKUST Mandarin Telephone Transcript Data.",no,"yes, HKUST Mandarin Telephone Speech",they used LDC Fisher Spanish Speech,no,"yes, Fisher Spanish Transcript and CALLHOME Spanish Transcript","yes, The CALLHOME Spanish Speech",no,no,WSJ0 text,yes WSJCAM0,no,no they are transcribing their own data,"yes, LJSpeech","yes, librovox Linda johnson's recording.",no,they used FastSpeech algorithm's output.,"yes CSTR VCTK, WaveNet(pre-trained) and Tacotron(pre-trained)",uses CSTR VCTK corpus with two pre trained models: WaveNet and Tacotron,no they are original,"yes, CSTR VCTK and LibriSpeech","yes, I was only able to find MagnaTagATune and DARPA TIMIT though","yes CSTR VCTK Corpus and
LibriSpeech ASR corpus","Yes, they linked to the audio book website LibriVox",no,no they are original,yes both Tag A Tune and Magna Tune.,yes MagnaTune music label and FreeSound,no they are original,no they are original