Reference for the resource,"@misc{amt,
	author = {},
	title = {{A}mazon {M}echanical {T}urk --- mturk.com},
	howpublished = {\url{https://www.mturk.com/}},
	year = {},
	note = {[Accessed 25-Jun-2023]},
}","@inproceedings{wsj023,
title={Deep clustering: Discriminative embeddings for segmentation and separation},
author={Hershey, John R and Chen, Zhuo and Le Roux, Jonathan and Watanabe, Shinji},
booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages={31--35},
year={2016},
organization={IEEE}
}","@article{aurora,
  title={Aurora working group: DSR front end LVCSR evaluation AU/384/02},
  author={Pearce, David and Picone, J},
  journal={Inst. for Signal \& Inform. Process., Mississippi State Univ., Tech. Rep},
  year={2002}
}","@inproceedings{libri,
  title={Librispeech: an asr corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5206--5210},
  year={2015},
  organization={IEEE}
}","@article{switchboard21,
  title={Switchboard-1 Release 2},
  author={Godfrey, John J and Holliman, Edward},
  journal={Linguistic Data Consortium, Philadelphia},
  volume={926},
  pages={927},
  year={1997}
}","@misc{ted21,
        author = {A. Rousseau, P. Deléglise, and Y. Estève},
        title = {TED-LIUMv2},
        howpublished = {\url{https://openslr.magicdatatech.com/19/}},
        year = {2014},
        note = {[Accessed 25-Jun-2023]},
}",\amt,"@inproceedings{wsjcam0,
  title={WSJCAMO: a British English speech corpus for large vocabulary continuous speech recognition},
  author={Robinson, Tony and Fransen, Jeroen and Pye, David and Foote, Jonathan and Renals, Steve},
  booktitle={1995 International Conference on Acoustics, Speech, and Signal Processing},
  volume={1},
  pages={81--84},
  year={1995},
  organization={IEEE}
}","@inproceedings{mcwsjav1,
  title={The multi-channel Wall Street Journal audio visual corpus (MC-WSJ-AV): Specification and initial experiments},
  author={Lincoln, Mike and McCowan, Iain and Vepa, Jithendra and Maganti, Hari Krishna},
  booktitle={IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.},
  pages={357--362},
  year={2005},
  organization={IEEE}
}","@misc{ljspeech,
  author       = {Keith Ito and Linda Johnson},
  title        = {The LJ Speech Dataset},
  howpublished = {\url{https://keithito.com/LJ-Speech-Dataset/}},
  year         = 2017
}",\amt,"@misc{magnatag1,
        author = {Edith Law },
        title = {{P}apers with {C}ode - {M}agna{T}ag{A}{T}une {D}ataset --- paperswithcode.com},
        howpublished = {\url{https://paperswithcode.com/dataset/magnatagatune}},
        year = {2019},
        note = {[Accessed 25-Jun-2023]},
}"
Another reference for the resource,\cite{imagenet1} \cite{imagenet2},,,,"@misc{switchboard22,
        author = {John J. Godfrey, Edward Holliman},
        title = {{S}witchboard-1 {R}elease 2 - {L}inguistic {D}ata {C}onsortium --- catalog.ldc.upenn.edu},
        howpublished = {\url{https://catalog.ldc.upenn.edu/LDC97S62}},
        year = {1997},
        note = {[Accessed 25-Jun-2023]},
}","@inproceedings{ted22,
  title={Enhancing the TED-LIUM corpus with selected data for language modeling and more TED talks.},
  author={Rousseau, Anthony and Del{\'e}glise, Paul and Esteve, Yannick and others},
  booktitle={LREC},
  pages={3935--3939},
  year={2014}
}",,,"@misc{mcwsjav2,
        author = {Erich Zwyssig, Mike Lincoln},
        title = {MC-WSJ-AV corpus --- catalog.ldc.upenn.edu},
        howpublished = {\url{https://catalog.ldc.upenn.edu/docs/LDC2014S03/README.pdf}},
        year = {2012},
        note = {[Accessed 25-Jun-2023]},
}",,,"@inproceedings{magnatag2,
  title={Evaluation of algorithms using games: The case of music tagging.},
  author={Law, Edith and West, Kris and Mandel, Michael I and Bay, Mert and Downie, J Stephen},
  booktitle={ISMIR},
  pages={387--392},
  year={2009},
  organization={Citeseer}
}"
Another reference for the resource,,,,,,,,,,,,"@misc{magnatag3,
        author = {Edith Law, Kris West, Michael Mandel, Mert Bay and J. Stephen Downie},
        title = {{T}he {M}agna{T}ag{A}{T}une {D}ataset | {C}ity {U}niversity {M}{I}{R}{G} --- mirg.city.ac.uk},
        howpublished = {\url{https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset}},
        year = {2009},
        note = {[Accessed 25-Jun-2023]},
}"
Name,Amazon Mechanical Turk for checking ImageNet,WSJ0-2/3mix,Aurora-4,LibriSpeech,Switchboard-1 Release 2,TED-LIUMv2,Amazon Mechanical Turk for ,WSJCAM0,MC-WSJ-AV,LJSpeech,AMT for evaluating the FastPitch algorithm,MagnaTagATune dataset
Type of Modification,Filtering,Mixing,Mixing and filtering,Further alignment,Correction,Further alignment and filtering.,Further annotation,Re-recording the audio,Re-recording the audio,Detailed annotating and alignment,Checking,Further annotation
Details of the modification,They checked the validity of the ImageNet dataset and filtered the data of ImageNet which was wrongly annotated.,They mixed the WSJ0 data. -2 refers to mixing two different datapoints of WSJ0 and 3 refers to mixing 3 different datapoints of WSJ0 to mimic 2 and 3 people speaking.,"They mixed WSJ0 data with random background sounds such as traffic sound, baby crying sound and some other random sounds (bird, dog etc) on the background of WSJ0 dataset. If I understood correctly, they also filtered according to the quality of the WJS0 datapoints.",They obtained and aligned the audio book data frim LibroVox. They have only selected 1000 hours of LibriVox data.,They have corrected the wrong alignments and annotations.,They obtained 207 hours of TED Talks' audio and text data. They aligned those properly so that it can be used as a training audio ML set.,They translated the Spanish annotations to English and added those as the further annotation to the dataset,They re-recorded the WSJ0's audio with British speakers. They kept the speech text the same.,They re-recorded WSJ0 speech data with very different microphone locations.,Keith Ito aligned LindaJohnson's LibriVox data. He further aligned the voice per words.,They used it to determine how good their algorithm sounds like.,They added tags in different parts of the songs and also to the songs themselves. They added the tags with by playing the game 'Tag a Tune' on MagnaTune label's song dataset.
Name of the modified dataset (the dataset that is modified to create this current dataset),ImageNet,WSJ0,WSJ0,LibriVox,SWITCHBOARD,TED Talks,CALLHOME Spanish Speech dataset,WSJ0,WSJ0,Linda Johnsons' LibroVox data,FastPitch algorithm's results,MagnaTuneDataset
Detail stating version,Explicit,Explicit,Implicit,Implicit,Implicit,Explicit,Implicit,Implicit,Implicit,Implicit,Explicit,Implicit
Justification for detail stating version,AMT expects the users a very detailed documentation and an instruction manual to do their job. They need examples for the job etc. The paper refering this procedure was quite clear on how the filtering has done. ,Their documentation was very clear on how and why the mixing was done. They also gave sufficient information on the actual dataset (WSJ0) that was mixed.,"I could not find where the noise data came from. They have a 53 paged document and the part where it was on the added noise, I could not find their datasource. I also could not find why they decided to use WSJ0 instead of another speech dataset. Due to those two reasons, implicit was chosen.  ",LibriVox is a very big library and people are uploading further data everyday. It is not stated in their documentation whose data or which books were considered to be included in the dataset. ,"They gave only a few examples on what they have corrected, there is no list or any detailed description on what the problems were. Their documentation is only a A4 paged long README.txt.",They gave information on the statistics of their dtaaset such as male and female speaker numbers. They also inform on why they have chosen TED Talks as a datasource. They inform on how they aligned the text and the audio as well.,Their documentation lacks on how the translation process happened and how the translations are presented in the dataset. There is also not much explanation on why this translation was necessary.,There is no information on where the speakers are from exactly. I also could not find whether they have re-recorded all of WSJ0 data.  ,They gave no information on who the speakers were and which part of the WSJ0 was re-recorded (because they recorded a very short version) and why WSJ0 text was chosen.,"Even though Keith Ito was very responsive in email, the document was not clear on what change he did to prepare the data further. I also did not see the details on why Linda Johnson was selected instead of other voice actors who read the books. Due to those lack of information in the documentation, it is classified as implicit.",They instructed how to rate their system and explained their metrics very well.,"They do not mention how they did it or how it was allowed, whetehr there is a checking mechanism for wrong tags. It was also not clear which songs of the label were considered."