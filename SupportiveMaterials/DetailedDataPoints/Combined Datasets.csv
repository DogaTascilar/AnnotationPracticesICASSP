Place I found the resource,"@misc{imagenet1,
        author = {},
        title = {imagenet-1k Â· {D}atasets at {H}ugging {F}ace --- huggingface.co},
        howpublished = {\url{https://huggingface.co/datasets/imagenet-1k }},
        year = {},
        note = {[Accessed 25-Jun-2023]},
}","@inproceedings{coco,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}","@inproceedings{aishell11,
  title={Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline},
  author={Bu, Hui and Du, Jiayu and Na, Xingyu and Wu, Bengu and Zheng, Hao},
  booktitle={2017 20th conference of the oriental chapter of the international coordinating committee on speech databases and speech I/O systems and assessment (O-COCOSDA)},
  pages={1--5},
  year={2017},
  organization={IEEE}
}","@misc{chime,
        author = {Kean Chin},
        title = {{T}he 4th {C}{H}i{M}{E} {S}peech {S}eparation and {R}ecognition {C}hallenge --- spandh.dcs.shef.ac.uk},
        howpublished = {\url{https://spandh.dcs.shef.ac.uk/chime_challenge/CHiME4/}},
        year = {2016},
        note = {[Accessed 25-Jun-2023]},
}","@article{fishhome1,
  title={Fisher and CALLHOME Spanish--English speech translation},
  author={Post, Matt and Kumar, Gaurav and Lopez, Adam and Karakos, Damianos and Callison-Burch, Chris and Khudanpur, Sanjeev},
  journal={LDC2014T23. Web Download. Philadelphia: Linguistic Data Consortium},
  year={2014}
}","@article{reverb1,
  title={A summary of the REVERB challenge: state-of-the-art and remaining challenges in reverberant speech processing research},
  author={Kinoshita, Keisuke and Delcroix, Marc and Gannot, Sharon and P. Habets, Emanu{\""e}l A and Haeb-Umbach, Reinhold and Kellermann, Walter and Leutnant, Volker and Maas, Roland and Nakatani, Tomohiro and Raj, Bhiksha and others},
  journal={EURASIP Journal on Advances in Signal Processing},
  volume={2016},
  pages={1--19},
  year={2016},
  publisher={Springer}
}","@inproceedings{ted3,
  title={TED-LIUM 3: Twice as much data and corpus repartition for experiments on speaker adaptation},
  author={Hernandez, Fran{\c{c}}ois and Nguyen, Vincent and Ghannay, Sahar and Tomashenko, Natalia and Esteve, Yannick},
  booktitle={Speech and Computer: 20th International Conference, SPECOM 2018, Leipzig, Germany, September 18--22, 2018, Proceedings 20},
  pages={198--208},
  year={2018},
  organization={Springer}
}","@article{asv1,
  title={ASVspoof 2019: A large-scale public database of synthesized, converted and replayed speech},
  author={Wang, Xin and Yamagishi, Junichi and Todisco, Massimiliano and Delgado, H{\'e}ctor and Nautsch, Andreas and Evans, Nicholas and Sahidullah, Md and Vestman, Ville and Kinnunen, Tomi and Lee, Kong Aik and others},
  journal={Computer Speech \& Language},
  volume={64},
  pages={101114},
  year={2020},
  publisher={Elsevier}
}"
Another place I found the resource,"@INPROCEEDINGS{imagenet2,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  doi={10.1109/CVPR.2009.5206848}}",,,,"@inproceedings{fishhome2,
      Title = {Improved Speech-to-Text Translation with the {F}isher and {C}allhome {S}panish--{E}nglish Speech Translation Corpus},
      Author = {Post, Matt and Kumar, Gaurav and Lopez, Adam and Karakos, Damianos and Callison-Burch, Chris and Khudanpur, Sanjeev},
      Booktitle = {Proceedings of the International Workshop on Spoken Language Translation (IWSLT)},
      Year = {2013},
      Address = {Heidelberg, Germany},
      Month = {December}
    }","@misc{reverb2,
        author = {Keisuke Kinoshita, Marc Delcroix, Takuya Yoshioka, Tomohiro Nakatani},
        title = {{T}he {R}{E}{V}{E}{R}{B} challenge - {E}valuating de-reverberation and {A}{S}{R} techniques in reverberant environments --- reverb2014.dereverberation.com},
        howpublished = {\url{https://reverb2014.dereverberation.com}},
        year = {2014},
        note = {[Accessed 25-Jun-2023]},
}",,"@misc{asv2,
        author = {Junichi Yamagishi},
        title = { | {A}{S}{V}spoof 2019 --- asvspoof.org},
        howpublished = {\url{https://www.asvspoof.org/index2019.html}},
        year = {2019},
        note = {[Accessed 25-Jun-2023]},
}"
Another place I found the resource,,,,,"@misc{fishhome3,
        author = {    Post, Matt and Kumar, Gaurav and Lopez, Adam and Karakos, Damianos and Callison-Burch, Chris and Khudanpur, Sanjeev},
        title = {{L}inguistic {D}ata {C}onsortium {C}atalog --- catalog.ldc.upenn.edu},
        howpublished = {\url{https://catalog.ldc.upenn.edu/docs/LDC2014T23/}},
        year = {},
        note = {[Accessed 25-Jun-2023]},
}",,,"@article{asv3,
  title={ASVspoof 2019: spoofing countermeasures for the detection of synthesized, converted and replayed speech},
  author={Nautsch, Andreas and Wang, Xin and Evans, Nicholas and Kinnunen, Tomi H and Vestman, Ville and Todisco, Massimiliano and Delgado, H{\'e}ctor and Sahidullah, Md and Yamagishi, Junichi and Lee, Kong Aik},
  journal={IEEE Transactions on Biometrics, Behavior, and Identity Science},
  volume={3},
  number={2},
  pages={252--265},
  year={2021},
  publisher={IEEE}
}"
Name,ImageNet,MS COCO,AISHELL-1,4th CHiME,Fisher and CALLHOME,The REVERB (REverberant Voice Enhancement and Recognition Benchmark) ,TED-LIUM Release 3,ASVspoof2019
Name of the combined datasets,"Flickr, WordNet, search engine photos","publicly available image repositories, individual photographers, search engines",AISHELL-ASR0009 and Kaldi HKUST,WSJ0 and Kaldi,LDC Fisher Spanish - Transcripts and The CALLHOME Spanish Transcripts,MC-WSJ-AV and WSJCAM0,TED-LIUMv2 and other TED Talks,"CSTR VCTK, WaveNet, Tacotron"
Combination explanation,They used WordNet to select words to search in Flickr and search engines to find relevant images by their tags. They checked their correctness by Amazon Mechanical Turk.,They have gathered images and combined them in one big dataset by making their formats compatible.,They filtered and highly edited AISHELL-ASR0009 and utilized Kaldi HKUST recipe to do those and also to test the corpus if I understood correctly.,They re-recorded some WSJ0's speech in noisy environments and they recorded some clean data (or used original WSJ0 data) and added noise with Kaldi. ,They joined those two datasets. They were compatible in terms of their formattings but they ,"They used both of those for different purposes as SimData and RealData. I think they combined and altered some of their data. If I understood correctly, they combined them in one dataset. ",They used all of TED-LIUMv2 and added the same amount of TED Talks.,uses CSTR VCTK corpus with two pre trained models: WaveNet and Tacotron.
Detail stating version,Explicit,Implicit,Implicit,Implicit,Explicit,Implicit,Explicit,Implicit
Justification for detail stating version,They were very clear on how their combination process happened and they were clearn on why they used the datasets and the engines they have utilized.,They did not give any proper sources on where and who they take their data. We assume it is due to copyright issues.,"They do not give much informationo on Kaldi HKUST and why using Kaldi's HKUST recipe is relevant such as what are the common traits between HKUST and AISHELL. I also could not find why they decided to filter the AISHELL-ASR0009. However, the main reason for classifying as Implicit was due to Kaldi HKUST.",It is not apparent which data they have used. They also do not say where the noise data come from such that they recorded it or they used a Kaldi recipe containing noise data. I also could not detect whether they recorded the clean data or used WSJ0 original.,"They were very open on how those initial datasets were collected and why they decided to combine so. Their reference links worked pretty well, as well.",They did not say why they have chosen those datasets and what those dataset contain properly. I also had troubles of understanding what they have done to create the REVERB dataset/challenge.,They were clear about the purpose of enhancing TED-LIUMv2 dataset. They stated how much new data has came. They also revisited their previous release and corrected some alignments. ,They were not clear that they were using pre-trained models of WaveNet and Tacotron 2. I needed to search those technologies and then understood that they were pre-trained. 