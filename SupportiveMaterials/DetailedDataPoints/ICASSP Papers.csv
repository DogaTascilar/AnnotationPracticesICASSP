Title,SA-Net: Shuffle attention for deep convolutional neural networks,Attention is all you need in speech separation,Recent developments on ESPNeT toolkit boosted by conformer,FastPitch: Parallel text-to-speech with pitch prediction,End-to-end anti-spoofing with rawnet2
DOI,10.1109/ICASSP39728.2021.9414568,10.1109/ICASSP39728.2021.9413901,10.1109/ICASSP39728.2021.9414858,10.1109/ICASSP39728.2021.9413889,10.1109/ICASSP39728.2021.9414234
Citation Count,156,121,80,66,62
Data coming from,"Uses ResNet, ImageNet1k and MS COCO",data from WSJo-2/3mix,trained on 17 datasets,takes one female reader data from LJSpeech which takes its data from the audio book website librivox.,"ASVspoof 2019 dataset, uses only LA part"
Stating style,Implicit,Explicit,Implicit,Implicit,Implicit
The reasoning for the stating style,They have mention the datasets in the abstract but no information was given or no paragraph was dedicated to explain what those dataset/models represent or why it was relevant to use those. No reference for MS COCO. ,"Very clearly stated in the abstract, dedicated the section '3.1 Dataset' on the dataset utilized. Explained the dataset characteristics. Went further and mentioned the actual original data source. ","Not stated in the abstract, first mention of those were in the tables. 10 out of 17 datasets' names were written incorrectly. Most of them were not referenced properly. Nothing was mentioned on the annotation practices of the datasets.   ",Not mentioned in abstract. Not dedicated a section nor an exclusive mention. Only proper mention was 2 sentences long and not gave much information on the content nor the annotation practices of the LJSpeech dataset. Needed to read all of the experimentation section to find the dataset. ,"I was not familiar with the topic and the technologies utilized so this was the hardest paper I needed to delve into. Mentioned in the abstract but I think it is talking about the ASV challenge itself rather than the dataset. Mentioned in the technologies utilized and the metrics utilized. No information on the content of the data, the information given was on the formatting of the data. "
Quick notes,"Uses ResNet, ImageNet1k and MS COCO","Uses WSJ0-2/3mix and they did not give any info related to the dataset used. They just said  ""hey we used this"" and there is no validation or reasoning why they decided using that instead of others.",They used 17 datasets to train and test their system. They wrote half of the dataset names incorrectly and not give any info on annotation practices or any content related to the datasets utilized.,trained on LJSpeech dataset and tested their system with AMT,only uses ASVspoof2019 dataset
(Optional) additional comments,-,-,Have inconsistencies with their datasets. More than one of their datasets uses WSJ0 and they did not mention. They have written the names of 10 out of 17 utilized datasets wrongly.,-,"I did not understand most of the paper. I tried to search the technologies but I am very unsure on their utilized technologies and techniques. I have never heard of their metrics and even though I searched, I still am unsure of them."
Q1:was the work an original task?,No,No,No,No,No
Q2: did they use human annotations as labels?,Yes,Yes,Yes,Yes,Yes
Q3:did they use original human annotations? (i.e. annotations they collected themselves?),No,No,No,Yes,No
Q4:did they use external human annotations? (i.e. annotations from an existing dataset?) note: it might that they used both their original annotations as well as external annotations,Yes,Yes,Yes,Yes,Yes
"Q5:who were the annotators? (e.g. paper's authors, mturk, experts etc. - i.e. what population were they drawn from?)",Not Given,Not Given,Not Given,Not Given,Not Given
Q6:did they specify the number of annotators?,No,Not Given,Not Given,No,No
Q6a: did they estimate how many annotators they would need beforehand?,No,No,Not Given,No,No
Q7: were there formal instructions for the annotators?,Not Given,Not Given,Not Given,"Yes for AMT, the speech data is taken from LJSpeech",Not Given
Q8: was there training for the annotators?,No,Not Given,Not Given,"Yes for AMT, the speech data is taken from LJSpeech",Not Given
Q9: was there prescreening on the crowdwork platforms?,Not Given,Not Given,Not Given,Yes,they did not use crowd platforms
Q10: was there multiple annotator overlap? (i.e. did multiple annotators label the same item?),Not Given,Not Given,Not Given,Not Given,Not Given
Q11:reported inter-annotator agreement?,Not Given,Not Given,Not Given,Not Given,Not Given
Q11a: (or any other metric of label quality?),Not Given,Not Given,Not Given,Not Given,Not Given
"Q12: did they link to the dataset? note: if they're using all external data, i.e. annotations and raw data, we'd like to track down the original reference and see if information on these questions is available","ImageNet, ResNet50, MS COCO","yes , WSJ0-2/3mix",They put in a table by writing more than half of their names incorrectly.,"yes, LJSpeech","yes CSTR VCTK, WaveNet(pre-trained) and Tacotron(pre-trained)"